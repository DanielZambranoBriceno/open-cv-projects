10: Contrastive Language-Image Pre-training (CLIP) is a model developed by OpenAI that learns to link images and text by training on a large dataset of image-text pairs. It uses a contrastive loss function to create similar embeddings for matching image-text pairs, enabling zero-shot image classification. CLIPâ€™s versatility allows it to perform tasks like image retrieval, text-to-image generation, and aesthetic ranking without task-specific fine-tuning.